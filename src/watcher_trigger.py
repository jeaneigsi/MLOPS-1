"""
watcher_trigger.py - Surveillance et d√©clenchement automatique de la pipeline

Ce script :
1. Surveille les nouvelles versions de dataset dans ClearML
2. D√©tecte les changements par rapport √† la derni√®re version trait√©e
3. D√©clenche automatiquement la pipeline de r√©entra√Ænement

Peut √™tre ex√©cut√© :
- Manuellement (python watcher_trigger.py)
- Via un cron job
- Comme t√¢che ClearML p√©riodique

Usage:
    # V√©rification unique:
    python src/watcher_trigger.py
    
    # Mode surveillance continue:
    python src/watcher_trigger.py --watch --interval 300
    
    # Force le d√©clenchement:
    python src/watcher_trigger.py --force
"""

import argparse
import time
import json
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, Any, List

from clearml import Task, Dataset

from utils import (
    setup_clearml_credentials,
    CLEARML_PROJECT_NAME,
    CLEARML_DATASET_FEEDBACK_NAME,
    PROJECT_ROOT
)


# Fichier pour stocker l'√©tat du dernier dataset trait√©
STATE_FILE = PROJECT_ROOT / ".watcher_state.json"


def parse_args():
    """Parse les arguments de la ligne de commande."""
    parser = argparse.ArgumentParser(description="Watcher pour d√©clenchement automatique de pipeline")
    
    parser.add_argument(
        "--watch", action="store_true",
        help="Mode surveillance continue"
    )
    parser.add_argument(
        "--interval", type=int, default=300,
        help="Intervalle de v√©rification en secondes (default: 300 = 5 min)"
    )
    parser.add_argument(
        "--force", action="store_true",
        help="Forcer le d√©clenchement m√™me si pas de nouveau dataset"
    )
    parser.add_argument(
        "--dry-run", action="store_true",
        help="Afficher ce qui serait fait sans ex√©cuter"
    )
    parser.add_argument(
        "--queue", type=str, default="default",
        help="Queue ClearML pour la pipeline (default: default)"
    )
    parser.add_argument(
        "--epochs", type=int, default=10,
        help="Nombre d'epochs pour le r√©entra√Ænement (default: 10)"
    )
    parser.add_argument(
        "--batch-size", type=int, default=32,
        help="Taille du batch (default: 32)"
    )
    
    return parser.parse_args()


# =============================================================================
# GESTION DE L'√âTAT
# =============================================================================

def load_state() -> Dict[str, Any]:
    """
    Charge l'√©tat du watcher depuis le fichier JSON.
    
    Returns:
        Dictionnaire avec l'√©tat
    """
    if not STATE_FILE.exists():
        return {
            "last_processed_id": None,
            "last_processed_name": None,
            "last_check": None,
            "history": []
        }
    
    with open(STATE_FILE, "r", encoding="utf-8") as f:
        return json.load(f)


def save_state(state: Dict[str, Any]):
    """
    Sauvegarde l'√©tat du watcher.
    
    Args:
        state: Dictionnaire d'√©tat √† sauvegarder
    """
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, indent=2, ensure_ascii=False)


def update_state(dataset_id: str, dataset_name: str, triggered: bool = True):
    """
    Met √† jour l'√©tat apr√®s traitement d'un dataset.
    
    Args:
        dataset_id: ID du dataset trait√©
        dataset_name: Nom du dataset
        triggered: Si True, une pipeline a √©t√© d√©clench√©e
    """
    state = load_state()
    
    state["last_processed_id"] = dataset_id
    state["last_processed_name"] = dataset_name
    state["last_check"] = datetime.now().isoformat()
    
    # Ajouter √† l'historique (garder les 10 derniers)
    state["history"].insert(0, {
        "dataset_id": dataset_id,
        "dataset_name": dataset_name,
        "timestamp": datetime.now().isoformat(),
        "triggered": triggered
    })
    state["history"] = state["history"][:10]
    
    save_state(state)


# =============================================================================
# D√âTECTION DES DATASETS
# =============================================================================

def get_all_feedback_datasets() -> List[Dict[str, Any]]:
    """
    R√©cup√®re tous les datasets de feedback du projet.
    
    Returns:
        Liste de datasets tri√©s par date (plus r√©cent en premier)
    """
    try:
        datasets = Dataset.list_datasets(
            dataset_project=CLEARML_PROJECT_NAME,
            partial_name=CLEARML_DATASET_FEEDBACK_NAME,
            only_completed=True
        )
        
        # Trier par date de cr√©ation
        sorted_datasets = sorted(
            datasets,
            key=lambda d: d.get("created", ""),
            reverse=True
        )
        
        return sorted_datasets
        
    except Exception as e:
        print(f"Erreur lors de la r√©cup√©ration des datasets: {e}")
        return []


def get_latest_dataset() -> Optional[Dict[str, Any]]:
    """
    R√©cup√®re le dataset le plus r√©cent.
    
    Returns:
        Dataset ou None
    """
    datasets = get_all_feedback_datasets()
    return datasets[0] if datasets else None


def check_for_new_dataset() -> Optional[Dict[str, Any]]:
    """
    V√©rifie s'il y a un nouveau dataset depuis le dernier traitement.
    
    Returns:
        Nouveau dataset ou None
    """
    state = load_state()
    last_processed_id = state.get("last_processed_id")
    
    latest = get_latest_dataset()
    
    if not latest:
        return None
    
    if latest["id"] != last_processed_id:
        return latest
    
    return None


# =============================================================================
# D√âCLENCHEMENT DE LA PIPELINE
# =============================================================================

def trigger_pipeline(
    dataset_id: str,
    queue: str = "default",
    epochs: int = 10,
    batch_size: int = 32,
    dry_run: bool = False
) -> Optional[str]:
    """
    D√©clenche la pipeline de r√©entra√Ænement.
    
    Args:
        dataset_id: ID du dataset
        queue: Queue ClearML
        epochs: Nombre d'epochs
        batch_size: Taille du batch
        dry_run: Si True, n'ex√©cute pas vraiment
        
    Returns:
        ID de la t√¢che pipeline ou None
    """
    print(f"\nüöÄ D√©clenchement de la pipeline de r√©entra√Ænement")
    print(f"   Dataset ID: {dataset_id}")
    print(f"   Queue: {queue}")
    print(f"   Epochs: {epochs}")
    print(f"   Batch size: {batch_size}")
    
    if dry_run:
        print("\n   [DRY RUN] Aucune action effectu√©e")
        return None
    
    try:
        # Option 1: Importer et appeler directement le module
        from pipeline_retrain import retrain_pipeline
        from clearml.automation import PipelineDecorator
        
        # Configurer la queue
        PipelineDecorator.set_default_execution_queue(queue)
        
        # Lancer la pipeline
        result = retrain_pipeline(
            dataset_id=dataset_id,
            epochs=epochs,
            batch_size=batch_size
        )
        
        print(f"\n‚úì Pipeline d√©clench√©e avec succ√®s")
        return result.get("model_id") if result else None
        
    except Exception as e:
        print(f"\n‚ùå Erreur lors du d√©clenchement: {e}")
        
        # Option 2: Alternative via Task.enqueue
        try:
            print("Tentative alternative via Task.enqueue...")
            
            # Cloner et ex√©cuter une t√¢che existante
            # (n√©cessite qu'une pipeline ait d√©j√† √©t√© ex√©cut√©e une fois)
            existing_tasks = Task.get_tasks(
                project_name=CLEARML_PROJECT_NAME,
                task_name="Image_Classification_AutoRetrain"
            )
            
            if existing_tasks:
                cloned = Task.clone(task=existing_tasks[0].id)
                cloned.set_parameters({"dataset_id": dataset_id})
                Task.enqueue(task=cloned, queue_name=queue)
                print(f"‚úì T√¢che clon√©e et mise en queue: {cloned.id}")
                return cloned.id
            else:
                print("‚ùå Aucune t√¢che pipeline existante trouv√©e")
                return None
                
        except Exception as e2:
            print(f"‚ùå Alternative √©chou√©e: {e2}")
            return None


# =============================================================================
# BOUCLE DE SURVEILLANCE
# =============================================================================

def run_check(args) -> bool:
    """
    Effectue une v√©rification unique.
    
    Returns:
        True si une pipeline a √©t√© d√©clench√©e
    """
    print(f"\n{'='*60}")
    print(f"V√âRIFICATION - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}")
    
    # Charger l'√©tat actuel
    state = load_state()
    last_id = state.get("last_processed_id", "Aucun")
    print(f"Dernier dataset trait√©: {last_id}")
    
    if args.force:
        print("\n‚ö†Ô∏è  Mode forc√©: r√©cup√©ration du dernier dataset...")
        new_dataset = get_latest_dataset()
    else:
        new_dataset = check_for_new_dataset()
    
    if not new_dataset:
        print("\n‚úì Pas de nouveau dataset d√©tect√©")
        return False
    
    print(f"\nüÜï Nouveau dataset d√©tect√©!")
    print(f"   ID: {new_dataset['id']}")
    print(f"   Nom: {new_dataset['name']}")
    print(f"   Cr√©√©: {new_dataset.get('created', 'N/A')}")
    
    # D√©clencher la pipeline
    task_id = trigger_pipeline(
        dataset_id=new_dataset["id"],
        queue=args.queue,
        epochs=args.epochs,
        batch_size=args.batch_size,
        dry_run=args.dry_run
    )
    
    # Mettre √† jour l'√©tat
    if not args.dry_run:
        update_state(
            dataset_id=new_dataset["id"],
            dataset_name=new_dataset["name"],
            triggered=task_id is not None
        )
    
    return True


def run_watch_loop(args):
    """
    Ex√©cute la boucle de surveillance continue.
    """
    print(f"\nüîÑ Mode surveillance activ√©")
    print(f"   Intervalle: {args.interval} secondes")
    print(f"   Appuyez sur Ctrl+C pour arr√™ter\n")
    
    try:
        while True:
            triggered = run_check(args)
            
            if triggered:
                print(f"\n‚è≥ Attente de {args.interval} secondes avant prochaine v√©rification...")
            else:
                print(f"‚è≥ Prochaine v√©rification dans {args.interval} secondes...")
            
            time.sleep(args.interval)
            
    except KeyboardInterrupt:
        print("\n\nüëã Arr√™t de la surveillance")


# =============================================================================
# AFFICHAGE DU STATUT
# =============================================================================

def show_status():
    """Affiche le statut actuel du watcher."""
    state = load_state()
    
    print("\nüìä STATUT DU WATCHER")
    print("=" * 40)
    print(f"Dernier dataset trait√©: {state.get('last_processed_name', 'Aucun')}")
    print(f"ID: {state.get('last_processed_id', 'N/A')}")
    print(f"Derni√®re v√©rification: {state.get('last_check', 'Jamais')}")
    
    history = state.get("history", [])
    if history:
        print(f"\nHistorique ({len(history)} derniers):")
        for entry in history[:5]:
            triggered = "‚úì" if entry.get("triggered") else "‚óã"
            print(f"  {triggered} {entry['dataset_name']} - {entry['timestamp'][:19]}")
    
    # Datasets disponibles
    print("\nüìÅ Datasets disponibles:")
    datasets = get_all_feedback_datasets()
    if datasets:
        for ds in datasets[:5]:
            marker = "‚Üí" if ds["id"] == state.get("last_processed_id") else " "
            print(f"  {marker} {ds['name']} ({ds['id'][:8]}...)")
    else:
        print("  Aucun dataset trouv√©")


# =============================================================================
# POINT D'ENTR√âE
# =============================================================================

def main():
    """Point d'entr√©e principal."""
    args = parse_args()
    
    # Configuration ClearML
    setup_clearml_credentials()
    
    # Initialiser une t√¢che ClearML si en mode surveillance
    if args.watch:
        task = Task.init(
            project_name=CLEARML_PROJECT_NAME,
            task_name="dataset_watcher",
            task_type=Task.TaskTypes.monitor
        )
    else:
        task = None
    
    print("=" * 60)
    print("WATCHER - D√âTECTION DE NOUVEAUX DATASETS")
    print("=" * 60)
    
    # Afficher le statut
    show_status()
    
    if args.watch:
        run_watch_loop(args)
    else:
        run_check(args)
    
    if task:
        task.close()


if __name__ == "__main__":
    main()
